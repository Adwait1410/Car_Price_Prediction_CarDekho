# -*- coding: utf-8 -*-
"""ml_final_fa.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k5Ol5o8DNS3Wcn1EfE1dtjt8VuKE5z4X
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

df=pd.read_csv("/content/cardekho_dataset.csv")

df

df.info()

df.columns

df.describe()

df.describe(include='object')

df.isnull().sum()

df.shape

df.nunique()

df.dtypes

df.duplicated().sum()

null_counts = df.isnull().sum()
print("Null values count per column:")
print(null_counts)


if null_counts.sum() > 0:
    data_no_nulls = df.dropna()
    print(f"Shape after dropping nulls: {data_no_nulls.shape}")
else:
    print("No null values found. No rows were removed.")
    data_no_nulls = df.copy()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer

numerical_cols = df.select_dtypes(include=np.number).columns

for col in numerical_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]

    print(f"Outliers in '{col}':")
    if not outliers.empty:
        display(outliers[[col]])
    else:
        print("No outliers found.")
    print("-" * 30)



df_no_outliers = df.copy()
for col in numerical_cols:
    Q1 = df_no_outliers[col].quantile(0.25)
    Q3 = df_no_outliers[col].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    df_no_outliers = df_no_outliers[(df_no_outliers[col] >= lower_bound) & (df_no_outliers[col] <= upper_bound)]

print("Shape of the dataframe after removing outliers:", df_no_outliers.shape)

numerical_cols = df.select_dtypes(include=np.number).columns
plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_cols):
    plt.subplot(3, 3, i + 1)
    sns.boxplot(x=df[col])
    plt.title(f'Box plot of {col}')
plt.tight_layout()
plt.show()

df.shape

df_no_outliers.shape

#Linear regression
# --- 1. Define Target and Features ---

# Target variable (what we want to predict)
y = df['selling_price']

# Feature matrix (what we use to make the prediction)
# Drop the target, the unnecessary index, and high-cardinality text features
X = df.drop([
    'selling_price',
    'Unnamed: 0',
    'car_name',
    'brand',
    'model'
], axis=1)

print("--- Features and Target defined ---")
print(f"Target (y): 'selling_price'")
print(f"Features (X) columns: {X.columns.tolist()}")

# --- 2. Define Preprocessing Steps ---

# Identify numerical and categorical features
numerical_features = [
    'vehicle_age',
    'km_driven',
    'mileage',
    'engine',
    'max_power',
    'seats'
]
categorical_features = [
    'seller_type',
    'fuel_type',
    'transmission_type'
]

# Create preprocessing pipeline for numerical features
# 1. StandardScaler: Scales data (e.g., mean=0, std=1)
numerical_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

# Create preprocessing pipeline for categorical features
# 1. OneHotEncoder: Converts categories into binary columns
categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combine preprocessing steps using ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ])

print("--- Preprocessing pipeline created ---")

# --- 3. Create and Train the Model ---

# Create the full pipeline: 1. Preprocess, 2. Run Linear Regression
model_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', LinearRegression())
])

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(f"Data split: {len(X_train)} training samples, {len(X_test)} testing samples")

# Train the model
model_pipeline.fit(X_train, y_train)
print("--- Model training complete ---")

# --- 4. Evaluate the Model ---

# Make predictions on the test set
y_pred = model_pipeline.predict(X_test)

# Calculate evaluation metrics
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print("\n--- REGRESSION MODEL RESULTS ---")
print(f"Target: Predict 'selling_price'")
print(f"Model: Linear Regression")
print("\nEvaluation Metrics:")
print(f"  Root Mean Squared Error (RMSE): {rmse:.2f}")
print(f"  R-squared (R2): {r2:.4f}")
print("\n(RMSE shows the model's average prediction error in the same units as the target, 'selling_price'.)")
print(f"(R2 score of {r2:.4f} means that {r2*100:.2f}% of the variance in 'selling_price' is predictable from the features.)")

plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.title('Actual vs. Predicted Selling Prices')
plt.xlabel('Actual Selling Price')
plt.ylabel('Predicted Selling Price')
plt.grid(True)
plt.show()

residuals = y_test - y_pred

plt.figure(figsize=(10, 6))
plt.scatter(y_pred, residuals, alpha=0.5)
plt.title('Residual Plot')
plt.xlabel('Predicted Selling Price')
plt.ylabel('Residuals (Actual - Predicted)')
plt.hlines(0, plt.xlim()[0], plt.xlim()[1], color='red', linestyle='--')
plt.grid(True)
plt.show()

from sklearn.ensemble import GradientBoostingRegressor

# Create the full pipeline: 1. Preprocess, 2. Run Gradient Boosting Regressor
model_pipeline_gbr = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', GradientBoostingRegressor(random_state=42)) # Added random_state for reproducibility
])

# Train the Gradient Boosting Regressor model
model_pipeline_gbr.fit(X_train, y_train)
print("--- Gradient Boosting Regressor Model training complete ---")

# Make predictions on the test set
y_pred_gbr = model_pipeline_gbr.predict(X_test)

# Calculate evaluation metrics
rmse_gbr = np.sqrt(mean_squared_error(y_test, y_pred_gbr))
r2_gbr = r2_score(y_test, y_pred_gbr)

print("\n--- GRADIENT BOOSTING REGRESSOR MODEL RESULTS ---")
print(f"Target: Predict 'selling_price'")
print(f"Model: Gradient Boosting Regressor")
print("\nEvaluation Metrics:")
print(f"  Root Mean Squared Error (RMSE): {rmse_gbr:.2f}")
print(f"  R-squared (R2): {r2_gbr:.4f}")
print("\n(Compare these metrics to the Linear Regression results to see if there's an improvement.)")



plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_gbr, alpha=0.5)
plt.title('Actual vs. Predicted Selling Prices (Gradient Boosting Regressor)')
plt.xlabel('Actual Selling Price')
plt.ylabel('Predicted Selling Price')
plt.grid(True)
plt.show()

from sklearn.tree import DecisionTreeRegressor

# Create the full pipeline for Decision Tree Regression
model_pipeline_dt = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', DecisionTreeRegressor(random_state=42))
])

# Train the Decision Tree Regression model
model_pipeline_dt.fit(X_train, y_train)
print("--- Decision Tree Regressor Model training complete ---")

# Make predictions on the test set
y_pred_dt = model_pipeline_dt.predict(X_test)

# Calculate evaluation metrics for Decision Tree
rmse_dt = np.sqrt(mean_squared_error(y_test, y_pred_dt))
r2_dt = r2_score(y_test, y_pred_dt)

print("\n--- DECISION TREE REGRESSOR MODEL RESULTS ---")
print(f"Target: Predict 'selling_price'")
print(f"Model: Decision Tree Regressor")
print("\nEvaluation Metrics:")
print(f"  Root Mean Squared Error (RMSE): {rmse_dt:.2f}")
print(f"  R-squared (R2): {r2_dt:.4f}")

plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_dt, alpha=0.5)
plt.title('Actual vs. Predicted Selling Prices (Decision Tree Regressor)')
plt.xlabel('Actual Selling Price')
plt.ylabel('Predicted Selling Price')
plt.grid(True)
plt.show()

from sklearn.ensemble import RandomForestRegressor

# Create the full pipeline for Random Forest Regression
model_pipeline_rf = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', RandomForestRegressor(random_state=42))
])

# Train the Random Forest Regression model
model_pipeline_rf.fit(X_train, y_train)
print("\n--- Random Forest Regressor Model training complete ---")

# Make predictions on the test set
y_pred_rf = model_pipeline_rf.predict(X_test)

# Calculate evaluation metrics for Random Forest
rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))
r2_rf = r2_score(y_test, y_pred_rf)

print("\n--- RANDOM FOREST REGRESSOR MODEL RESULTS ---")
print(f"Target: Predict 'selling_price'")
print(f"Model: Random Forest Regressor")
print("\nEvaluation Metrics:")
print(f"  Root Mean Squared Error (RMSE): {rmse_rf:.2f}")
print(f"  R-squared (R2): {r2_rf:.4f}")

plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_rf, alpha=0.5)
plt.title('Actual vs. Predicted Selling Prices (Random Forest Regressor)')
plt.xlabel('Actual Selling Price')
plt.ylabel('Predicted Selling Price')
plt.grid(True)
plt.show()

from sklearn.linear_model import Ridge, Lasso

# Create the full pipeline for Ridge Regression
model_pipeline_ridge = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', Ridge(random_state=42))
])

# Train the Ridge Regression model
model_pipeline_ridge.fit(X_train, y_train)
print("--- Ridge Regression Model training complete ---")

# Make predictions on the test set
y_pred_ridge = model_pipeline_ridge.predict(X_test)

# Calculate evaluation metrics for Ridge
rmse_ridge = np.sqrt(mean_squared_error(y_test, y_pred_ridge))
r2_ridge = r2_score(y_test, y_pred_ridge)

print("\n--- RIDGE REGRESSION MODEL RESULTS ---")
print(f"Target: Predict 'selling_price'")
print(f"Model: Ridge Regression")
print("\nEvaluation Metrics:")
print(f"  Root Mean Squared Error (RMSE): {rmse_ridge:.2f}")
print(f"  R-squared (R2): {r2_ridge:.4f}")

# Create the full pipeline for Lasso Regression
model_pipeline_lasso = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', Lasso(random_state=42))
])

# Train the Lasso Regression model
model_pipeline_lasso.fit(X_train, y_train)
print("\n--- Lasso Regression Model training complete ---")

# Make predictions on the test set
y_pred_lasso = model_pipeline_lasso.predict(X_test)

# Calculate evaluation metrics for Lasso
rmse_lasso = np.sqrt(mean_squared_error(y_test, y_pred_lasso))
r2_lasso = r2_score(y_test, y_pred_lasso)

print("\n--- LASSO REGRESSION MODEL RESULTS ---")
print(f"Target: Predict 'selling_price'")
print(f"Model: Lasso Regression")
print("\nEvaluation Metrics:")
print(f"  Root Mean Squared Error (RMSE): {rmse_lasso:.2f}")
print(f"  R-squared (R2): {r2_lasso:.4f}")



plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_ridge, alpha=0.5)
plt.title('Actual vs. Predicted Selling Prices (Ridge Regression)')
plt.xlabel('Actual Selling Price')
plt.ylabel('Predicted Selling Price')
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_lasso, alpha=0.5)
plt.title('Actual vs. Predicted Selling Prices (Lasso Regression)')
plt.xlabel('Actual Selling Price')
plt.ylabel('Predicted Selling Price')
plt.grid(True)
plt.show()

#classification

